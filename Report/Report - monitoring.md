# All at once clone detector
1. I was able to process the entire of the Corpus in ca 4 hours. If I were to guess what the fault could be had I crashed then I would lean towards memory overload. With a chunksize of 20 we need to indentify 18 milion clones. That is quit a lot of space and most of it will be in ram during the processing. But this is just a guess from observing my own computer during the processing. 
2. 
    - From the graph generated by the monitoring software we can see a linear behavior in the identification of the chunks. ![Chunks](./Chunks.png) This is the expected behavior as once we start indentifying chunks we need to compare them to the other chunks to find out if it is a new chunk or a copy of an already identified one. 
    - Sadly I do not have the data for how long exactly it took to create the candidates however based on when the last chunks was created and all the first entry for the candidates was we get a time of ca 25 min. Then the candidates are consumed as they are converted into clones. ![Candidates](./Candidates.png) We can note however that they are consumed in a linear fashion. I would expect the candidates to be somewhat reliant on the chunks allthough I don't know what that relation is, it might be linear, exponential or logarithmic for all i know. 
    - The average clone size I got was 52.24313929730379 by using the `Mongo aggregation 1`. If we know the average clone size we estimate how many clones we should get and with that provide an estimated timeline in the form of number of found clones vs the estimated amount of clones. 
    - The average number of chunks per file I got was 139.4700139470014 by using `Mongo aggregation 2` and piping the output to `Mongo aggregation 3`. By running the aggreration as two different stages it is possible to parallelize the operations. 

Get avrage clone size
> Mongo aggregation 1
```
$group: {
  _id: "Null",
  "avrage_size": {
    $avg: {
       $subtract : [ {$arrayElemAt: ["$instances.endLine", 0]}, 
                    {$arrayElemAt: ["$instances.startLine", 0]}]
    }
  }
}
```


Get average number of chunks per file, in two stages
> Mongo aggregation 2
```
$group: {
  _id: "$fileName",
  amount: {
    $count: {}
  }
}
```
Först säknar vi hur många document som har den fil namnet
> Mongo aggregation 3
```
$group: {
  _id: "Null",
  average: {
    $avg: "$amount"
  }
}
```
Sedan tar vi avrage på det. 